# -*- coding: utf-8 -*-
"""20101050_Fashion_MNIST_Student_Version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SDI1YY9TfKf_r3ysPbji61PSKHq0OxTF
"""

import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from keras.datasets import fashion_mnist
(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()
#t b

fig = plt.figure(figsize=(10,7))

for i in range(15):
    ax = fig.add_subplot(3, 5, i+1)
    ax.imshow(train_X[i], cmap= 'viridis')
    ax.set_title('Label (y): {y}'.format(y=train_y[i]))
    plt.axis('off')

"""Preprocessing:
1. Flatten the X
2. Convert the Y to one hot vector

Train:
1. Use Predicted Y= Weights * X
2. Apply softmax activation function on Predicted Y
3. Update the W, b using:

Weight=Weight-learning rate *(1/Number of rows in X train)* np.dot(np.transpose(X train),(predicted y-actual y))

b=b-learning rate*(1/Number of rows in X train)*(np.sum(predicted y-actual y))
"""

train_X.shape

#solve the same problem by flatten X, initialize W and b. Then do X*W+b and then find the error using softmax regression. Do everything manually

num_examples = train_X.shape[0]
num_pixels = train_X.shape[1] * train_X.shape[2]


#Flatten X Train
X_train=train_X.reshape(num_examples,num_pixels)
train_X_flattened=X_train/255
print(train_X_flattened.shape)

#Initialize W, b

num_classes = 10
W = np.random.randn(num_pixels, num_classes)
b = np.zeros(num_classes)

W.shape

b.shape

#Find the predicted y

def scored(W,b,train_X_flattened):
  scores = np.dot(train_X_flattened, W) + b      # y= W * X + b
  return scores

#Apply softmax to get the actual predicted y

def softmax(z):
  exp= np.exp(z-np.max(z))
  for i in range(len(z)):
    exp[i]/=np.sum(exp[i])
  return exp

train_y

num_examples = train_y.shape[0]
num_classes = np.max(train_y) + 1

one_hot= np.zeros((num_examples, num_classes))
one_hot[np.arange(num_examples), train_y] = 1

m=train_X.shape[0]
lamda=0.9


for i in range(1000):
  print(i)
  score = scored(W,b,train_X_flattened)
  predicted = softmax(score)

  W=W-lamda*(1/m)*np.dot(np.transpose(train_X_flattened),(predicted-one_hot))
  b=b-lamda*(1/m)*(np.sum(predicted-one_hot))

print(W)
print(b)

"""**Test**"""

test_X.shape

xtest=test_X.reshape(test_X.shape[0],test_X.shape[1]*test_X.shape[2])
xtest=xtest/255

test_predict= scored(W,b,xtest)
test_predict_softmax= softmax(test_predict)
test_predict_softmax

m=np.zeros(test_X.shape[0])

for i in range(test_X.shape[0]):
  m[i]=np.argmax(test_predict_softmax[i])


true=0
for i in range(test_X.shape[0]):
  if m[i]==test_y[i]:
    true+=1

#accuracy

print(true/test_X.shape[0])